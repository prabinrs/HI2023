---
lang-ref: ch.15
title: Week 15
---

## Lecture part A


As pointed out already, we can broadly classify Energy Based Models into generative or joint-embedding based on architectures and into contrastive or regularised & architectural based on training methods.

<!---
in the following ways based on training methods and architectures for multimodal prediction

 either generative of joint-embedding, they are either contrastive or regularised and/or architectural.

| EBMS       |  Training Methods          | Architectures  |
| ------------- |:-------------:| :-----:|
| 1.      | Contrastive methods | Latent variable models |
| 2.      | Regularized & Architectural methods      |   Latent variable models |
| 3.      | Contrastive methods | Joint embedding architectures |
| 4.      | Regularized & Architectural methods      | Joint embedding architectures |
--->

<!---
[comment]: <>(1. Contrastive methods, Latent variable models)
[comment]: <>(2. Regularized & Architectural methods, Latent variable models)
[comment]: <>(3. Contrastive methods, Joint embedding architectures)
[comment]: <>(4. Regularized & Architectural methods, Joint embedding architectures)
---->

In this section, we discussed Visual Representation Learning, focused on self-supervised visual representation learning. This can be classified into Generative models, Pretext Tasks and Joint Embedding methods. In generative models, you train the model to reconstruct the original image from the noisy image. In pretext tasks, you train the model to figure out a smart way to generate pseudo labels. Joint Embedding methods try to make their backbone network robust to certain distortions and are invariant to data augmentation. JEM training methods can be classified into four types: contrastive methods, non-contrastive methods, clustering methods and Other methods. He concluded the lecture by discussing contrastive methods which push positive pairs closer and negative pairs away. 
<!--
These models require negative samples and finding them becomes difficult as the embedding spaces become large. This problem can be solved by using a memory bank: use a small batch size but instead of using negative samples from only the current batch, collect them even from previous batches. 
-->
 

## Lecture part B

In this section, we discussed non-contrastive methods which are based on information theory and donâ€™t require special architectures or engineering techniques. Then, he went on to discuss clustering methods which prevent trivial solution by quantizing the embedding space. Finally, he discussed "Other" methods which are local and don't create problem with distributed training unlike previous methods. He concluded the lecture by suggesting various improvisations for JEMs w.r.t Data augmentation and network architecture. 

<!--
He mentioned that random crop is the most critical one and that people are moving towards masking augmentation which works well for transformer type of architecture. He finally concluded the lecture with the discussion about projector/expander that can improve the training if added to the network architecture.
-->
